{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "source": [
    "print('starting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MIMIC dataprep\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_file_path = '../data/'\n",
    "file_path = initial_file_path + 'LABEVENTS.csv'\n",
    "\n",
    "labDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()\n",
    "\n",
    "# print(labDF.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = initial_file_path + 'D_LABITEMS.csv'\n",
    "\n",
    "lab_indexDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58976\n"
     ]
    }
   ],
   "source": [
    "file_path = initial_file_path + 'ADMISSIONS.csv'\n",
    "\n",
    "admitDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()\n",
    "\n",
    "print(admitDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = initial_file_path + 'PATIENTS.csv'\n",
    "\n",
    "demoDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = initial_file_path + 'D_ICD_DIAGNOSES.csv'\n",
    "\n",
    "diag_indexDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = initial_file_path + 'DIAGNOSES_ICD.csv'\n",
    "\n",
    "diagDF = spark.read.csv(file_path, sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Processing admit features #####\n",
    "DF_admit= admitDF.join(demoDF,on='SUBJECT_ID',how='left') \\\n",
    "    .withColumn('DOB',col('DOB').cast(TimestampType())) \\\n",
    "    .withColumn('ADMITTIME',col('ADMITTIME').cast(TimestampType())) \\\n",
    "    .withColumn('DISCHTIME',col('DISCHTIME').cast(TimestampType())) \\\n",
    "    .withColumn('HADM_ID',col('HADM_ID').cast(StringType())) \\\n",
    "\n",
    "    \n",
    "udf_gender=udf(lambda e: 0 if (e == 'M') else 1,IntegerType())\n",
    "udf_urgent=udf(lambda e: 1 if (e == 'URGENT') else 0,IntegerType())\n",
    "udf_emer=udf(lambda e: 1 if (e == 'EMERGENCY') else 0,IntegerType())\n",
    "\n",
    "\n",
    "DF_admit=DF_admit \\\n",
    ".withColumn('GENDER', udf_gender(col('GENDER'))) \\\n",
    ".withColumn('age',year(col('ADMITTIME')) - year(col('DOB'))) \\\n",
    ".withColumn('age',col('age').cast(IntegerType())) \\\n",
    ".withColumn('admit_LOS',datediff(col('DISCHTIME') , col('ADMITTIME'))) \\\n",
    ".withColumn('is_urgent',udf_urgent(col('ADMISSION_TYPE'))) \\\n",
    ".withColumn('is_emergency',udf_emer(col('ADMISSION_TYPE'))) \\\n",
    ".drop(('ROW_ID'))\n",
    "\n",
    "\n",
    "# Filter\n",
    "DF_admit=DF_admit.filter(col('age') > 16).filter(col('age') < 100)\\\n",
    "# .filter(col('HADM_ID'),isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_admit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48231\n"
     ]
    }
   ],
   "source": [
    "# DF_admit.take(1)\n",
    "# DF_admit.printSchema()\n",
    "print(DF_admit.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Processing lab features #####\n",
    "\n",
    "# df_labs=labs.merge(labs_index,on='ITEMID',how='left')\n",
    "labsJoinedDF= labDF.join(lab_indexDF,on='ITEMID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27854055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labsJoinedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creatinine|Bicarbonate|Hematocrit|Hemoglobin|Potassium|Sodium|Ammonia|Albumin|Magnesium|Urea Nitrogen|Chloride|Amylase|Asparate Aminotransferase (AST)|WBC|Temperature|pH|pO2|pCO2|Glucose|Protein|Triglycerides|Globulin\n"
     ]
    }
   ],
   "source": [
    "a_labels=['Creatinine',\n",
    "          'Bicarbonate',\n",
    "          'Hematocrit',\n",
    "          'Hemoglobin',\n",
    "          'Potassium',\n",
    "          'Sodium',\n",
    "            #     'Bilirubin',\n",
    "          'Ammonia',\n",
    "          'Albumin',\n",
    "          'Magnesium',\n",
    "          'Urea Nitrogen',\n",
    "            #     'Calcium',\n",
    "          'Chloride',\n",
    "          'Amylase',\n",
    "          'Asparate Aminotransferase (AST)',\n",
    "          'WBC', \n",
    "          'Temperature',\n",
    "          'pH',\n",
    "          'pO2',\n",
    "          'pCO2',\n",
    "          'Glucose',\n",
    "          'Protein',\n",
    "          'Triglycerides',\n",
    "          'Globulin'\n",
    "    ]\n",
    "\n",
    "labrexp='|'.join(a_labels)\n",
    "print(labrexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labDF.\n",
    "# a_labels=labsJoinedDF.select('LABEL').collect()\n",
    "# res=a_labels.LABEL.str.contains(labrexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(a_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_filter = udf(lambda e: e in a_labels)\n",
    "# labFeatsDF = labJoinedDF[res]\n",
    "\n",
    "labFeatsDF = labsJoinedDF.filter(col('LABEL').isin(a_labels)) \\\n",
    "    .withColumn('labelname',col('LABEL')) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labFeatsDF=labFeatsDF.join(admitDF.select('ADMITTIME','DISCHTIME','HADM_ID'),on='HADM_ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10672272"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labFeatsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labFeatsDF = labFeatsDF \\\n",
    "    .withColumn('CHARTTIME',col('CHARTTIME').cast(TimestampType())) \\\n",
    "    .withColumn('LOS',datediff(col('CHARTTIME') ,col('ADMITTIME'))) \\\n",
    "    .withColumn('day',col('LOS').cast(StringType())) \\\n",
    "    .withColumn('HADM_ID',col('HADM_ID').cast(StringType())) \\\n",
    "    .withColumn('hadm_los',concat(col('HADM_ID'),lit('_'),col('LOS'))) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10672272"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labFeatsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labFeatsDF = labFeatsDF.filter(col('HADM_ID').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labFeatsDF.take(1)\n",
    "# labFeatsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Labs processing #####\n",
    "# import pyspark.sql.functions as F\n",
    "# # from pyspark.sql.functions import rowNumber\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # w = Window().partitionBy(\"id_sa\").orderBy(col(\"cnt\").desc())\n",
    "\n",
    "# # labFeatsDF.partitionBy(['hadm_los','labelname']).orderby(col('CHARTTIME').desc())\n",
    "\n",
    "# maxs=labFeatsDF.groupBy(\"hadm_los\",\"labelname\").agg(F.max(\"CHARTTIME\").alias(\"mx\"))\n",
    "# max_labs_DF=labFeatsDF.join(maxs, \n",
    "#   (col(\"CHARTTIME\") == col(\"mx\"))\n",
    "# )\n",
    "\n",
    "# idx=df_nan_labs.groupBy(['hadm_los','labelname'])['CHARTTIME'].transform(max) == df_nan_labs['CHARTTIME']\n",
    "# df_max_labs=df_nan_labs[idx]\n",
    "\n",
    "# idx2=df_max_labs.groupby(['hadm_los','labelname'])['ROW_ID_x'].transform(max) == df_max_labs['ROW_ID_x']\n",
    "# df_max2_labs=df_max_labs[idx2]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_labs_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w3 = Window.partitionBy(\"hadm_los\", \"labelname\").orderBy(desc(\"CHARTTIME\"))\n",
    "featsDF = labFeatsDF.select('*', rank().over(w3).alias('rank')) \\\n",
    "  .filter(col('rank') < 2) \n",
    "#   .show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+----------+-------------------+-----+--------+--------+----+------+-----+-----+----------+----------+---------+-------------------+-------------------+---+---+--------+----+\n",
      "|HADM_ID|ITEMID|  ROW_ID|SUBJECT_ID|          CHARTTIME|VALUE|VALUENUM|VALUEUOM|FLAG|ROW_ID|LABEL|FLUID|  CATEGORY|LOINC_CODE|labelname|          ADMITTIME|          DISCHTIME|LOS|day|hadm_los|rank|\n",
      "+-------+------+--------+----------+-------------------+-----+--------+--------+----+------+-----+-----+----------+----------+---------+-------------------+-------------------+---+---+--------+----+\n",
      "| 100003| 51491|21885288|     54610|2150-04-17 22:01:00|  5.5|     5.5|   units|null|   691|   pH|Urine|Hematology|    5803-2|       pH|2150-04-17 15:34:00|2150-04-21 17:30:00|  0|  0|100003_0|   1|\n",
      "+-------+------+--------+----------+-------------------+-----+--------+--------+----+------+-----+-----+----------+----------+---------+-------------------+-------------------+---+---+--------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# featsDF.count()\n",
    "featsDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot so we have a column per lab\n",
    "daily_labs_DF=featsDF.groupBy('hadm_los').pivot('labelname').agg(last('VALUENUM'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524765\n"
     ]
    }
   ],
   "source": [
    "print(daily_labs_DF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_ham=udf(lambda e: e.split('_')[0])\n",
    "udf_los=udf(lambda e: e.split('_')[1])\n",
    "\n",
    "daily_labs_DF=daily_labs_DF \\\n",
    "    .withColumn('HADM_ID',udf_ham(col('hadm_los'))) \\\n",
    "    .withColumn('LOS',udf_los(col('hadm_los'))) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hadm_los: string (nullable = true)\n",
      " |-- Albumin: string (nullable = true)\n",
      " |-- Ammonia: string (nullable = true)\n",
      " |-- Amylase: string (nullable = true)\n",
      " |-- Asparate Aminotransferase (AST): string (nullable = true)\n",
      " |-- Bicarbonate: string (nullable = true)\n",
      " |-- Chloride: string (nullable = true)\n",
      " |-- Creatinine: string (nullable = true)\n",
      " |-- Globulin: string (nullable = true)\n",
      " |-- Glucose: string (nullable = true)\n",
      " |-- Hematocrit: string (nullable = true)\n",
      " |-- Hemoglobin: string (nullable = true)\n",
      " |-- Magnesium: string (nullable = true)\n",
      " |-- Potassium: string (nullable = true)\n",
      " |-- Protein: string (nullable = true)\n",
      " |-- Sodium: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Triglycerides: string (nullable = true)\n",
      " |-- Urea Nitrogen: string (nullable = true)\n",
      " |-- WBC: string (nullable = true)\n",
      " |-- pCO2: string (nullable = true)\n",
      " |-- pH: string (nullable = true)\n",
      " |-- pO2: string (nullable = true)\n",
      " |-- HADM_ID: string (nullable = true)\n",
      " |-- LOS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_labs_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+----+----+----+----+-------+---+\n",
      "|hadm_los|Albumin|Ammonia|Amylase|Asparate Aminotransferase (AST)|Bicarbonate|Chloride|Creatinine|Globulin|Glucose|Hematocrit|Hemoglobin|Magnesium|Potassium|Protein|Sodium|Temperature|Triglycerides|Urea Nitrogen| WBC|pCO2|  pH| pO2|HADM_ID|LOS|\n",
      "+--------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+----+----+----+----+-------+---+\n",
      "|100001_0|   null|   null|   null|                             16|         18|     114|       2.3|    null|    141|      32.2|        11|      1.9|      4.1|    300|   144|       null|         null|           36|   2|null| 7.5|null| 100001|  0|\n",
      "|100001_1|   null|   null|   null|                           null|         20|     109|       2.1|    null|    264|        32|      11.2|      2.2|      4.4|    100|   142|       null|         null|           23|   1|null| 6.5|null| 100001|  1|\n",
      "|100001_2|   null|   null|   null|                           null|         23|     107|         2|    null|    185|      null|      null|        2|      3.3|    300|   138|       null|         null|           20|   2|null|   7|null| 100001|  2|\n",
      "|100001_3|   null|   null|   null|                           null|         23|     107|       2.1|    null|     65|      29.2|        10|        2|      4.1|    300|   138|       null|         null|           18|   1|null| 7.5|null| 100001|  3|\n",
      "|100001_4|   null|   null|   null|                           null|         23|     104|         2|    null|     66|      32.2|      10.9|        2|      3.5|   null|   138|       null|         null|           17|null|null|null|null| 100001|  4|\n",
      "+--------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+----+----+----+----+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_labs_DF.where(col('HADM_ID') == '100001').orderBy('LOS').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hematocrit': 31.26933838192296, 'Amylase': 106.19542577994893, 'Asparate Aminotransferase (AST)': 138.8242838208272, 'Magnesium': 2.0318854587049846, 'Chloride': 103.6867390270662, 'Temperature': 37.18157011042203, 'WBC': 19.51009535655058, 'Triglycerides': 163.66817564508827, 'Hemoglobin': 10.461022227971277, 'Ammonia': 59.942885440739516, 'pCO2': 42.42785544204624, 'Globulin': 2.748986301369863, 'pO2': 113.8872822935269, 'Bicarbonate': 25.641860099792897, 'Sodium': 138.60713253151383, 'Protein': 82.53410283315844, 'Urea Nitrogen': 28.501929200926018, 'Creatinine': 1.4545759030635566, 'Albumin': 2.954614999564725, 'pH': 7.016327225961998, 'Potassium': 4.112344059107788, 'Glucose': 130.01655345091677}\n"
     ]
    }
   ],
   "source": [
    "# Imputing values\n",
    "# impute mean\n",
    "# exclude list\n",
    "l_ex=['hadm_los','LOS','HADM_ID']\n",
    "\n",
    "a_stats=[]\n",
    "def fill_with_mean(df, exclude=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c in a_labels\n",
    "    ))\n",
    "    a_stats=stats.first().asDict()\n",
    "    print(a_stats)\n",
    "#     return df.na.fill(ct())\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "df=daily_labs_DF\n",
    "# print([e for e in df.columns not in l_ex])\n",
    "# labs_=[e for e in df.columns not in l_ex]\n",
    "df=df.select(*(col(c).cast(FloatType()) for c in a_labels))\n",
    "df_labs_imputed=fill_with_mean(daily_labs_DF, l_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one hot encoding representation\n",
    "DF_labs_imputed=df_labs_imputed\n",
    "# def set_one(e):\n",
    "#     if e != 0:\n",
    "#         return 1\n",
    "    \n",
    "# udf_set_one=udf(lambda e: set_one(e))\n",
    "# DF_diag_group=DF_diag_pivot.na.fill(0)\n",
    "DF_labsnew1=daily_labs_DF.select(*(min(col(c)).alias('min_'+c) for c in DF_labs_imputed.columns))\n",
    "DF_labsnew2=daily_labs_DF.select(*(max(col(c)).alias('max_'+c) for c in DF_labs_imputed.columns))\n",
    "DF_labsnew3=daily_labs_DF.select(*(mean(col(c)).alias('mean_'+c) for c in DF_labs_imputed.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  min_hadm_los min_Albumin min_Ammonia min_Amylase  \\\n",
      "0     100001_0          .9           1           0   \n",
      "\n",
      "  min_Asparate Aminotransferase (AST) min_Bicarbonate min_Chloride  \\\n",
      "0                                   0              10          100   \n",
      "\n",
      "  min_Creatinine min_Globulin min_Glucose   ...   min_Sodium min_Temperature  \\\n",
      "0            .05           .3        -251   ...          100               0   \n",
      "\n",
      "  min_Triglycerides min_Urea Nitrogen min_WBC min_pCO2 min_pH min_pO2  \\\n",
      "0                 1                 0      .1        0      0       0   \n",
      "\n",
      "  min_HADM_ID min_LOS  \n",
      "0      100001      -1  \n",
      "\n",
      "[1 rows x 25 columns]\n",
      "  max_hadm_los max_Albumin max_Ammonia max_Amylase  \\\n",
      "0     199999_6         6.9          99         999   \n",
      "\n",
      "  max_Asparate Aminotransferase (AST) max_Bicarbonate max_Chloride  \\\n",
      "0                                 999             9.2           99   \n",
      "\n",
      "  max_Creatinine max_Globulin max_Glucose   ...   max_Sodium max_Temperature  \\\n",
      "0            9.9          9.1         998   ...           97            43.7   \n",
      "\n",
      "  max_Triglycerides max_Urea Nitrogen max_WBC max_pCO2 max_pH max_pO2  \\\n",
      "0               999                99      99       99      9      99   \n",
      "\n",
      "  max_HADM_ID max_LOS  \n",
      "0      199999      99  \n",
      "\n",
      "[1 rows x 25 columns]\n",
      "  mean_hadm_los  mean_Albumin  mean_Ammonia  mean_Amylase  \\\n",
      "0          None      2.954615     59.942885    106.195426   \n",
      "\n",
      "   mean_Asparate Aminotransferase (AST)  mean_Bicarbonate  mean_Chloride  \\\n",
      "0                            138.824284          25.64186     103.686739   \n",
      "\n",
      "   mean_Creatinine  mean_Globulin  mean_Glucose    ...     mean_Sodium  \\\n",
      "0         1.454576       2.748986    130.016553    ...      138.607133   \n",
      "\n",
      "   mean_Temperature  mean_Triglycerides  mean_Urea Nitrogen   mean_WBC  \\\n",
      "0          37.18157          163.668176           28.501929  19.510095   \n",
      "\n",
      "   mean_pCO2   mean_pH    mean_pO2  mean_HADM_ID  mean_LOS  \n",
      "0  42.427855  7.016327  113.887282  150013.64907  9.877145  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_const=pd.concat(DF_labsnew1.toPandas(),DF_labsnew2.toPandas())\n",
    "# df_const.head(2)\n",
    "print(DF_labsnew1.toPandas())\n",
    "print(DF_labsnew2.toPandas())\n",
    "print(DF_labsnew3.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Save constants\n",
    "# # udf(lambda )\n",
    "\n",
    "# df_consts=daily_labs_DF.select(*daily_labs_DF.columns)\n",
    "# df_consts.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## filter numeric cols\n",
    "# num_cols = [col_type[0] for col_type in filter(lambda dtype: dtype[1] in {\"bigint\", \"double\", \"int\"}, daily_labs_DF.dtypes)]\n",
    "# ### Compute a dict with <col_name, median_value>\n",
    "# median_dict = dict()\n",
    "# for c in num_cols:\n",
    "#     median_dict[c] = daily_labs_DF.stat.approxQuantile(c, [0.5], 0.001)[0]\n",
    "\n",
    "# df_imputed = daily_labs_DF.na.fill(median_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs_=['Albumin', 'Ammonia', 'Bicarbonate',  'Chloride', 'Creatinine', 'Globulin', \\\n",
    "#         'Glucose', 'Hematocrit', 'Magnesium', 'Potassium', 'Protein', 'Sodium', 'Temperature', 'Urea Nitrogen', \n",
    "#        'WBC', 'pH', 'pO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_=a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to normalize\n",
    "\n",
    "# # inormalize\n",
    "# a_stats=[]\n",
    "# def fill_with_mean(df, exclude=set()): \n",
    "#     stats = df.agg(*(\n",
    "#         udf_norm(c,df_minmax(c),df_minmax(c)).alias(c) for c in df.columns if c in exclude\n",
    "#     ))\n",
    "#     a_stats=stats\n",
    "#     return df.na.fill(stats.first().asDict())\n",
    "\n",
    "# df_imputed=fill_with_mean(df, labs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # impute with median\n",
    "# df=daily_labs_DF.select(*(col(c).cast(FloatType()) for c in labs_))\n",
    "\n",
    "# num_cols = [col_type[0] for col_type in filter(lambda dtype: dtype[1] in {\"bigint\", \"double\", \"int\",\"float\"}, df.dtypes)]\n",
    "# num_cols=['Albumin', 'Ammonia', 'Bicarbonate']\n",
    "# median_dict = dict()\n",
    "# mean_dict = dict()\n",
    "# # print(df.dtypes)\n",
    "# for c in num_cols:\n",
    "#     \"\"\n",
    "#     median_dict[c] = df.stat.approxQuantile(c, [0.5], 0.001)[0]\n",
    "#     mean_dict[c] = avg(col('cb'))[0]\n",
    "    \n",
    "\n",
    "# # df_imputed = df.na.fill(median_dict)\n",
    "# print(median_dict)\n",
    "# print(mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524765"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Imputing with mean\n",
    "# # import pandas as pd\n",
    "# # df_means=pd.DataFrame()\n",
    "\n",
    "# def impute_mean(df):\n",
    "#     res = df.copy()\n",
    "# #     df_means=pd.DataFrame()\n",
    "    \n",
    "#     for feature_name in df.columns:\n",
    "#         if feature_name in a_labels:\n",
    "#             mean_value = df[feature_name].mean()\n",
    "            \n",
    "#             # fill with mean, alternative with median\n",
    "#             res[feature_name]=df[feature_name].fillna(mean_value)\n",
    "#             df_means[feature_name]=[mean_value]\n",
    "# #             print(mean_value)\n",
    "# #     print(df_means)\n",
    "#     return res\n",
    "\n",
    "# # df_daily_labs_imputed=impute_mean(df_daily_labs)\n",
    "# # DF_labs_imputed = impute_mean(DF_daily_labs)\n",
    "\n",
    "# udf_impute_mean=udf(impute_mean,DoubleType())\n",
    "\n",
    "# DF_labs_imputed = daily_labs_DF.select(*udf_daily_labs_DF.columns)\n",
    "# DF_labs_imputed = daily_labs_DF\n",
    "DF_labs_imputed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CCSD9s processing #####\n",
    "ccs9_dict = spark.read.csv(initial_file_path + 'ccs9_dict.csv', sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()\n",
    "    \n",
    "ccs9_icd9_mapping = spark.read.csv(initial_file_path + 'ccs9_icd9_mapping.csv', sep=',', header=True, nullValue='NULL', \n",
    "                          ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True) \\\n",
    "    .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs9_data = ccs9_dict.join(ccs9_icd9_mapping, on='CCS9', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_diag = diagDF.join(diag_indexDF,on='ICD9_CODE',how='left')\n",
    "# do an inner join with ccs9_data so that we only keep the diagnoses we are interested in\n",
    "DF_diag = DF_diag.join(ccs9_data, DF_diag.LONG_TITLE==ccs9_data.ICD9DSC, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516711"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF_diag.take(1)\n",
    "DF_diag.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w4 = Window.partitionBy(\"HADM_ID\", \"CCS9\").orderBy(desc('SUBJECT_ID'))\n",
    "\n",
    "diagfeatsDF = DF_diag.select('*', rank().over(w4).alias('rank')) \\\n",
    "  .filter(col('rank') < 2) \n",
    "\n",
    "DF_diag_pivot=diagfeatsDF.groupBy('HADM_ID').pivot( 'CCS9').agg(first('CCS9'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_values = {column: DF_diag_pivot.agg({column:\"mean\"}).flatMap(list).collect()[0] for column in DF_diag_pivot.columns if column not in ['HADM_ID']}\n",
    "# df_data = df_data.na.fill(fill_values)\n",
    "\n",
    "# fill_values = {column: 0 for column in DF_diag_pivot.columns if column not in ['HADM_ID']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_diag_group.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one hot encoding representation\n",
    "\n",
    "def set_one(e):\n",
    "    if (e == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "udf_set_one=udf(lambda e: set_one(e),IntegerType())\n",
    "\n",
    "fill_values = {column: 0 for column in DF_diag_pivot.columns if column not in ['HADM_ID']}\n",
    "DF_diag_group=DF_diag_pivot.na.fill(fill_values)\n",
    "# DF_diag_group=DF_diag_group.select(*(udf_set_one(col(c)).alias(c) for c in DF_diag_group.columns if c not in ['HADM_ID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_diag_group.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+-------------------+---------+--------------+--------------------+------------------+---------+--------+-------------+--------------+---------+-------------------+-------------------+-----------+--------------------+--------------------+------+-------------------+----+--------+-------+-----------+---+---------+---------+------------+-----------+\n",
      "|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|DISCHARGE_LOCATION|INSURANCE|LANGUAGE|     RELIGION|MARITAL_STATUS|ETHNICITY|          EDREGTIME|          EDOUTTIME|  DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|GENDER|                DOB| DOD|DOD_HOSP|DOD_SSN|EXPIRE_FLAG|age|admit_LOS|is_urgent|is_emergency|readmission|\n",
      "+----------+-------+-------------------+-------------------+---------+--------------+--------------------+------------------+---------+--------+-------------+--------------+---------+-------------------+-------------------+-----------+--------------------+--------------------+------+-------------------+----+--------+-------+-----------+---+---------+---------+------------+-----------+\n",
      "|     10096| 182988|2127-06-21 22:19:00|2127-06-24 15:51:00|     null|     EMERGENCY|EMERGENCY ROOM ADMIT|              HOME| Medicare|    ENGL|NOT SPECIFIED|       MARRIED|    WHITE|2127-06-21 21:12:00|2127-06-21 22:30:00|CHESST PAIN|                   0|                   1|     0|2051-07-15 00:00:00|null|    null|   null|          0| 76|        3|        0|           1|          0|\n",
      "+----------+-------+-------------------+-------------------+---------+--------------+--------------------+------------------+---------+--------+-------------+--------------+---------+-------------------+-------------------+-----------+--------------------+--------------------+------+-------------------+----+--------+-------+-----------+---+---------+---------+------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# admitDF.groupBy('SUBJECT_ID').orderBy('ADMITTIME').\n",
    "from pyspark.sql.window import Window\n",
    "w = Window().partitionBy(\"SUBJECT_ID\").orderBy(\"ADMITTIME\")\n",
    "\n",
    "# udf_con=udf(lambda e: 1 if e >= 0 else 0)\n",
    "udf_con=udf(lambda e: 1 if e > 0 else 0,IntegerType())\n",
    "\n",
    "labelDF = DF_admit \\\n",
    "    .withColumn(\"next_admit\", lead(col(\"ADMITTIME\"), 1).over(w)) \\\n",
    "    .withColumn('readmission_duration', datediff(col('next_admit'),col('DISCHTIME')))\n",
    "\n",
    "labelDF = labelDF.fillna({'readmission_duration':0})\n",
    "    \n",
    "labelDF = labelDF \\\n",
    "    .withColumn('readmission',udf_con(col('readmission_duration'))) \\\n",
    "    .drop(('next_admit')) \\\n",
    "    .drop(('readmission_duration')) \\\n",
    "\n",
    "labelDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelDF=labelDF.select('HADM_ID','readmission')\n",
    "df = labelDF.select('HADM_ID','readmission').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(labelDF.groupBy('HADM_ID').agg(first('HADM_ID')).select('HADM_ID').collect()))\n",
    "# df[df.SUBJECT_ID == '10111']\n",
    "# labelDF.drop(('next_admit')).drop(('readmission_duration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelDF.where(col('readmission') == 1).groupBy('HADM_ID').agg(first('HADM_ID')).select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelDF.where(col('readmission') == 0).groupBy('HADM_ID').agg(first('HADM_ID')).select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_admit2 = DF_admit.select('HADM_ID','SUBJECT_ID','GENDER','age','admit_LOS','is_urgent','is_emergency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_admit2.where(col('SUBJECT_ID')=='9150').collect()\n",
    "# DF_admit2.where(col('HADM_ID') == 120283).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out=outDF.select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genreate labels\n",
    "\n",
    "DF_y=labelDF.select('HADM_ID','readmission')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478262\n"
     ]
    }
   ],
   "source": [
    "# Joining different sets\n",
    "outDF=DF_labs_imputed.join(DF_admit2,on='HADM_ID',how='inner') \\\n",
    "    .join(DF_diag_group,on='HADM_ID',how='inner')\\\n",
    "    .join(DF_y,on='HADM_ID',how='left')\\\n",
    "    .drop('hadm_los')\\\n",
    "    .drop('SUBJECT_ID')\n",
    "\n",
    "print(outDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+---+----+---+---+---+------+---+---------+---------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----------+\n",
      "|HADM_ID|Albumin|Ammonia|Amylase|Asparate Aminotransferase (AST)|Bicarbonate|Chloride|Creatinine|Globulin|Glucose|Hematocrit|Hemoglobin|Magnesium|Potassium|Protein|Sodium|Temperature|Triglycerides|Urea Nitrogen|WBC|pCO2| pH|pO2|LOS|GENDER|age|admit_LOS|is_urgent|is_emergency| 10|100|101|102|103|104|105|106|107|108|109| 11|110|111|112|113|114|115|116|117|118|119| 12|120|121|122|123|124|125|126|127|128|129| 13|130|131|132|133|134|135|136|137|138|139| 14|140|141|142|143|144|145|146|147|148|149| 15|151|152|153|154|155|156|157|158|159| 16|160|161|162|163|164|165|166|168| 17|170|171|173|175|178| 18|181| 19|195|197|198|199|  2|200|201|202|203|204|205|206|207|208|209|210|211|212|213|215|217| 22|224|225|226|228|229| 23|230|231|232|233|234|235|236|237|238|239| 24|240|241|242|243|244|245|246|247|248|249| 25|250|251|252|253|255|256|257|259| 26|2603|2607|2608|2613|2615|2616|2617|2618|2619|2620|2621| 27| 29|  3| 32| 33| 35| 36| 37| 38| 39|  4| 40| 42| 43| 44| 45| 46| 47| 48| 49|  5| 50| 51| 52| 53| 54| 55| 57| 58| 59|  6| 60| 62| 63| 64|650|651|652|653|654|655|657|658|659|660|661|662|663|670|  7| 76| 77| 78| 79|  8| 80| 81| 82| 83| 84| 85| 86| 87| 88| 89| 90| 91| 92| 93| 94| 95| 96| 97| 98| 99|readmission|\n",
      "+-------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+---+----+---+---+---+------+---+---------+---------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----------+\n",
      "|      0|      0|      0|      0|                              0|          0|       0|         0|       0|      0|         0|         0|        0|        0|      0|     0|          0|            0|            0|  0|   0|  0|  0|  0|     0|  0|        0|        0|           0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|          0|\n",
      "+-------+-------+-------+-------+-------------------------------+-----------+--------+----------+--------+-------+----------+----------+---------+---------+-------+------+-----------+-------------+-------------+---+----+---+---+---+------+---+---------+---------+------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in outDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HADM_ID',\n",
       " 'Albumin',\n",
       " 'Ammonia',\n",
       " 'Amylase',\n",
       " 'Asparate Aminotransferase (AST)',\n",
       " 'Bicarbonate',\n",
       " 'Chloride',\n",
       " 'Creatinine',\n",
       " 'Globulin',\n",
       " 'Glucose',\n",
       " 'Hematocrit',\n",
       " 'Hemoglobin',\n",
       " 'Magnesium',\n",
       " 'Potassium',\n",
       " 'Protein',\n",
       " 'Sodium',\n",
       " 'Temperature',\n",
       " 'Triglycerides',\n",
       " 'Urea Nitrogen',\n",
       " 'WBC',\n",
       " 'pCO2',\n",
       " 'pH',\n",
       " 'pO2',\n",
       " 'LOS',\n",
       " 'GENDER',\n",
       " 'age',\n",
       " 'admit_LOS',\n",
       " 'is_urgent',\n",
       " 'is_emergency',\n",
       " '10',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '15',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '16',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '168',\n",
       " '17',\n",
       " '170',\n",
       " '171',\n",
       " '173',\n",
       " '175',\n",
       " '178',\n",
       " '18',\n",
       " '181',\n",
       " '19',\n",
       " '195',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '2',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '215',\n",
       " '217',\n",
       " '22',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '228',\n",
       " '229',\n",
       " '23',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '24',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '25',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '259',\n",
       " '26',\n",
       " '2603',\n",
       " '2607',\n",
       " '2608',\n",
       " '2613',\n",
       " '2615',\n",
       " '2616',\n",
       " '2617',\n",
       " '2618',\n",
       " '2619',\n",
       " '2620',\n",
       " '2621',\n",
       " '27',\n",
       " '29',\n",
       " '3',\n",
       " '32',\n",
       " '33',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '4',\n",
       " '40',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '6',\n",
       " '60',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '650',\n",
       " '651',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '657',\n",
       " '658',\n",
       " '659',\n",
       " '660',\n",
       " '661',\n",
       " '662',\n",
       " '663',\n",
       " '670',\n",
       " '7',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '8',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " 'readmission']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(outDF.columns))\n",
    "outDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data split\n",
    "demo=demoDF.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splitting and saving data\n",
    "# import numpy as np\n",
    "# np.random.seed(5)\n",
    "\n",
    "# # demo = demoDF.toPandas()\n",
    "\n",
    "\n",
    "# # split up the patients with 2/3 in training set and 1/6 in validation and test\n",
    "# train, validation, test = np.split(demo.SUBJECT_ID.sample(frac=1), [int(0.667*demo.shape[0]), int(0.833*demo.shape[0])])\n",
    "# train_patients = train.to_frame()\n",
    "# validation_patients = validation.to_frame()\n",
    "# test_patients = test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._conf.set('spark.executor.memory','32g').set('spark.driver.memory','32g').set('spark.driver.maxResultsSize','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed_path = '../data_processed/'\n",
    "tempdir = data_processed_path + 'tempdatadir.csv'\n",
    "outDF.coalesce(1).write.format(\"com.databricks.spark.csv\") \\\n",
    "   .mode('overwrite') \\\n",
    "   .option(\"header\", \"true\") \\\n",
    "   .save(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal dataset\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "tempcsv_file = [f for f in listdir(tempdir) if f[-4:] == '.csv'][0]\n",
    "# df_out=pd.read_csv(data_processed_path + 'mydata.csv/*.csv',header='infer')\n",
    "df_out=pd.read_csv(tempdir + '/' + tempcsv_file ,header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############3 Overlap with prediction_dataprep ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look back to only last 15 days\n",
    "n_lookback=10\n",
    "\n",
    "df_lookback=df_out.sort_values(['HADM_ID','LOS']).groupby('HADM_ID').tail(n_lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.DataFrame()\n",
    "\n",
    "def pad_frame(g):\n",
    "#     print(len(g))\n",
    "#     print(g)\n",
    "    if len(g) < n_lookback:\n",
    "        \"\"\n",
    "        offset=n_lookback - len(g)\n",
    "        df=g.iloc[0:1]\n",
    "#         print('offset',offset)\n",
    "#         print(df)\n",
    "        df_pad = df.loc[df.index.repeat(offset)]\n",
    "#         df_pad=pd.concat([df]*offset, ignore_index=True)\n",
    "#         print(len(df_pad))\n",
    "#         df_padded[0:offset]=g.iloc[0]\n",
    "        df2=pd.concat([df_pad,g]).reset_index().drop('index',axis=1)\n",
    "\n",
    "#         df_new=pd.concat([df_new,df2])\n",
    "#         print(len(df2))\n",
    "#         print(df2)\n",
    "#         print(len(df_new))\n",
    "        dfo=df2.sort_values('LOS',ascending=False)\n",
    "#         print(dfo)\n",
    "        return dfo\n",
    "    else:\n",
    "#         df_new=pd.concat([df_new,g])\n",
    "        dfo=g.sort_values('LOS', ascending=False)\n",
    "        return dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install  joblib --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "\n",
    "# def tmpFunc(df):\n",
    "#     df['c'] = df.a + df.b\n",
    "#     return df\n",
    "\n",
    "# def applyParallel(dfGrouped, func):\n",
    "#     print('numjobs',multiprocessing.cpu_count())\n",
    "#     retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "#     return pd.concat(retLst)\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# # df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\n",
    "# print('parallel version: ')\n",
    "# # print(applyParallel(df.groupby(df.index), tmpFunc))\n",
    "\n",
    "\n",
    "# df_group=applyParallel(df.groupby('HADM_ID'), pad_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(5)\n",
    "# # split up the patients with 2/3 in training set and 1/6 in validation and test\n",
    "# train, validation, test = np.split(df_admit.HADM_ID.sample(frac=1), [int(0.667*df_admit.shape[0]), int(0.833*df_admit.shape[0])])\n",
    "# train_patients = train.to_frame()\n",
    "# validation_patients = validation.to_frame()\n",
    "# test_patients = test.to_frame()\n",
    "\n",
    "# print(train_patients.shape)\n",
    "# print(validation_patients.shape)\n",
    "# print(test_patients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating the data split\n",
    "\n",
    "# train_patients['HADM_ID']=train_patients.HADM_ID.map(str)\n",
    "# validation_patients['HADM_ID']=validation_patients.HADM_ID.map(str)\n",
    "# test_patients['HADM_ID']=test_patients.HADM_ID.map(str)\n",
    "\n",
    "# # create the datasets based on hamid\n",
    "# train_simple = df_outy.merge(train_patients, on='HADM_ID', how='inner')\n",
    "# validation_simple = df_outy.merge(validation_patients, on='HADM_ID', how='inner')\n",
    "# test_simple  = df_outy.merge(test_patients, on='HADM_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# labels = df_train.groupby(['HADM_ID']).first()[['readmission']].as_matrix()\n",
    "# labels = np.squeeze(labels)\n",
    "# y_train = labels\n",
    "# print(y_train.shape)\n",
    "\n",
    "# labels = df_test.groupby(['HADM_ID']).first()[['readmission']].as_matrix()\n",
    "# labels = np.squeeze(labels)\n",
    "# y_test = labels\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_minmax=pd.read_csv('df_minmax.csv',header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs_=['ALT', 'AST', 'Albumin', 'Ammonia', 'Bicarbonate', 'Bilirubin', 'Calcium', 'Chloride', 'Creatinine', 'Globulin', \\\n",
    "#         'Glucose', 'Hematocrit', 'Magnesium', 'Potassium', 'Protein', 'Sodium', 'Temperature', 'Triglyceride', 'Urea Nitrogen', \n",
    "#        'WBC', 'pH', 'pO2', 'LOS','admit_LOS','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # labs_=df_minmax.columns.values[2:].filter(lambda e: e =='Unnamed: 24')\n",
    "# print(labs_)\n",
    "# def normalize(df):\n",
    "#     result = df.copy()\n",
    "#     for feature_name in df.columns:\n",
    "#         if feature_name in features_to_use:\n",
    "# #             print(feature_name)\n",
    "#             # min\n",
    "#             # mean\n",
    "#             # max\n",
    "#             min_value = df_minmax[feature_name][0]\n",
    "#             max_value = df_minmax[feature_name][1]\n",
    "#             #mean_value = df_mean[feature_name][2]\n",
    "#             result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "#     return result\n",
    "\n",
    "# df_1=df_train.copy()\n",
    "# df_2=df_test.copy()\n",
    "# df_1[labs_]=normalize(df_train[labs_])\n",
    "# df_2[labs_]=normalize(df_test[labs_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b=True\n",
    "# if (b):\n",
    "#     np.save('X_testd',X_test)\n",
    "#     np.save('y_testd',y_test)\n",
    "#     np.save('X_traind',X_train)\n",
    "#     np.save('y_traind',y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files\n",
    "\n",
    "# Batching with labs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching with labs and ccds9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31028, 1)\n",
      "(7723, 1)\n",
      "(7769, 1)\n",
      "(46520, 3)\n",
      "(46520,)\n",
      "(58976,)\n",
      "(58976, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "# split up the patients with 2/3 in training set and 1/6 in validation and test\n",
    "train, validation, test = np.split(demo.SUBJECT_ID.sample(frac=1), [int(0.667*demo.shape[0]), int(0.833*demo.shape[0])])\n",
    "train_patients = train.to_frame()\n",
    "validation_patients = validation.to_frame()\n",
    "test_patients = test.to_frame()\n",
    "\n",
    "print(train_patients.shape)\n",
    "print(validation_patients.shape)\n",
    "print(test_patients.shape)\n",
    "\n",
    "\n",
    "# for each patient, select a random admission from which to make a prediction\n",
    "dataf_admit=pd.read_csv(initial_file_path + 'ADMISSIONS.csv')\n",
    "admissions_to_predict_from = dataf_admit[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME']].groupby('SUBJECT_ID') \\\n",
    "            .apply(lambda x: x.iloc[np.random.choice(range(0,len(x)))]) \\\n",
    "            [['ADMITTIME','HADM_ID']].reset_index()\n",
    "\n",
    "# print(admissions_to_predict_from)\n",
    "print(admissions_to_predict_from.shape)\n",
    "print(dataf_admit.SUBJECT_ID.unique().shape)\n",
    "print(dataf_admit.HADM_ID.unique().shape)\n",
    "print(dataf_admit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478262, 261)\n",
      "(418628, 259)\n",
      "417834\n",
      "65577\n"
     ]
    }
   ],
   "source": [
    "model_data = df_out.merge(dataf_admit[['ADMITTIME', 'HADM_ID', 'SUBJECT_ID']], on='HADM_ID', how='inner')\n",
    "model_data = model_data.merge(admissions_to_predict_from[['SUBJECT_ID','ADMITTIME']], on='SUBJECT_ID', how='inner')\n",
    "print(model_data.shape)\n",
    "# since we are trying to predict what happens after ADMITTIME, only keep rows <= ADMITTIME\n",
    "model_data = model_data[model_data.ADMITTIME_x <= model_data.ADMITTIME_y]\n",
    "model_data.SUBJECT_ID = model_data.SUBJECT_ID.astype(str)\n",
    "del model_data['ADMITTIME_x']\n",
    "del model_data['ADMITTIME_y']\n",
    "print(model_data.shape)\n",
    "\n",
    "def update_colname(col):\n",
    "    if col.isdigit():\n",
    "        return 'dx' + col\n",
    "    col = col.replace(\" \", \"\")\n",
    "    col = col.replace(\"(\", \"\")\n",
    "    col = col.replace(\")\", \"\")\n",
    "    return col\n",
    "        \n",
    "model_data.columns = [update_colname(col) for col in model_data.columns]\n",
    "\n",
    "readmission_true_vals = model_data.merge(admissions_to_predict_from, on='HADM_ID', how='inner') \\\n",
    "    .groupby(['HADM_ID']) \\\n",
    "    .agg({'readmission':np.max, 'SUBJECT_ID_y':np.max}).reset_index()\n",
    "readmission_true_vals.rename(columns={'readmission':'readmission_true', 'SUBJECT_ID_y':'SUBJECT_ID'}, inplace=True)\n",
    "readmission_true_vals.SUBJECT_ID = readmission_true_vals.SUBJECT_ID.astype(str)\n",
    "del readmission_true_vals['HADM_ID']\n",
    "\n",
    "model_data = model_data.merge(readmission_true_vals, on='SUBJECT_ID', how='inner')\n",
    "del model_data['readmission']\n",
    "model_data.rename(columns={'readmission_true':'readmission'}, inplace=True)\n",
    "\n",
    "print(model_data.HADM_ID.count())\n",
    "print(model_data.readmission.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276420, 259)\n",
      "(69480, 259)\n",
      "(71934, 259)\n"
     ]
    }
   ],
   "source": [
    "# create the lab datasets\n",
    "train_final_seq = model_data.merge(train_patients, on='SUBJECT_ID', how='inner')\n",
    "validation_final_seq = model_data.merge(validation_patients, on='SUBJECT_ID', how='inner')\n",
    "test_final_seq = model_data.merge(test_patients, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(train_final_seq.shape)\n",
    "print(validation_final_seq.shape)\n",
    "print(test_final_seq.shape)\n",
    "train_final_seq.to_csv(data_processed_path + 'train_final_seq.csv', index=False)\n",
    "validation_final_seq.to_csv(data_processed_path + 'validation_final_seq.csv', index=False)\n",
    "test_final_seq.to_csv(data_processed_path + 'test_final_seq.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>dx231_mean</th>\n",
       "      <th>dx146_mean</th>\n",
       "      <th>dx119_mean</th>\n",
       "      <th>dx85_mean</th>\n",
       "      <th>dx87_mean</th>\n",
       "      <th>Chloride_mean</th>\n",
       "      <th>dx105_mean</th>\n",
       "      <th>dx2613_mean</th>\n",
       "      <th>dx2618_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>dx109_mean</th>\n",
       "      <th>is_urgent</th>\n",
       "      <th>dx123_mean</th>\n",
       "      <th>dx2603_mean</th>\n",
       "      <th>dx160_mean</th>\n",
       "      <th>dx153_mean</th>\n",
       "      <th>dx7_mean</th>\n",
       "      <th>pH_mean</th>\n",
       "      <th>dx40_mean</th>\n",
       "      <th>readmission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.250000</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.283164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.540541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.284611</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.852440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.468674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.049164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.710842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.985957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SUBJECT_ID  dx231_mean  dx146_mean  dx119_mean  dx85_mean  dx87_mean  \\\n",
       "0        100         0.0         0.0         0.0        0.0        0.0   \n",
       "1       1000         0.0         0.0         0.0        0.0        0.0   \n",
       "2      10000         0.0         0.0         0.0        0.0        0.0   \n",
       "3      10003       231.0         0.0         0.0        0.0        0.0   \n",
       "4      10004         0.0         0.0         0.0        0.0        0.0   \n",
       "\n",
       "   Chloride_mean  dx105_mean  dx2613_mean  dx2618_mean     ...       \\\n",
       "0     104.250000       105.0          0.0          0.0     ...        \n",
       "1     101.540541         0.0          0.0          0.0     ...        \n",
       "2     101.428571         0.0          0.0          0.0     ...        \n",
       "3      98.468674         0.0          0.0          0.0     ...        \n",
       "4     105.710842         0.0          0.0          0.0     ...        \n",
       "\n",
       "   dx109_mean  is_urgent  dx123_mean  dx2603_mean  dx160_mean  dx153_mean  \\\n",
       "0         0.0          0         0.0          0.0         0.0         0.0   \n",
       "1         0.0          1         0.0          0.0         0.0       153.0   \n",
       "2         0.0          0         0.0          0.0         0.0         0.0   \n",
       "3         0.0          0         0.0          0.0         0.0         0.0   \n",
       "4         0.0          0         0.0       2603.0         0.0         0.0   \n",
       "\n",
       "   dx7_mean   pH_mean  dx40_mean  readmission  \n",
       "0       0.0  7.283164        0.0            0  \n",
       "1       0.0  7.284611       40.0            0  \n",
       "2       0.0  6.852440        0.0            0  \n",
       "3       0.0  7.049164        0.0            0  \n",
       "4       0.0  6.985957        0.0            1  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict = {}\n",
    "agg_dict['readmission'] = np.max\n",
    "agg_dict['LOS'] = np.max\n",
    "agg_dict['GENDER'] = np.max\n",
    "agg_dict['age'] = np.max\n",
    "agg_dict['is_urgent'] = np.max\n",
    "agg_dict['is_emergency'] = np.max\n",
    "lab_tests_to_include = ['Albumin','Ammonia','Amylase','AsparateAminotransferaseAST','Bicarbonate','Chloride','Creatinine','Globulin','Glucose','Hematocrit','Hemoglobin','Magnesium','Potassium','Protein','Sodium','Temperature','Triglycerides','UreaNitrogen','WBC','pCO2','pH','pO2']\n",
    "# lab_dx_to_include = lab_tests_to_include + ['dx2','dx3','dx4','dx5','dx6','dx7','dx8','dx10','dx11','dx12','dx13','dx14','dx15','dx16','dx17','dx18','dx19','dx22','dx23','dx24','dx25','dx26','dx27','dx29','dx32','dx33','dx35','dx36','dx37','dx38','dx39','dx40','dx42','dx43','dx44','dx45','dx46','dx47','dx48','dx49','dx50','dx51','dx52','dx53','dx54','dx55','dx57','dx58','dx59','dx60','dx62','dx63','dx64','dx76','dx77','dx78','dx79','dx80','dx81','dx82','dx83','dx84','dx85','dx86','dx87','dx88','dx89','dx90','dx91','dx92','dx93','dx94','dx95','dx96','dx97','dx98','dx99','dx100','dx101','dx102','dx103','dx104','dx105','dx106','dx107','dx108','dx109','dx110','dx111','dx112','dx113','dx114','dx115','dx116','dx117','dx118','dx119','dx120','dx121','dx122','dx123','dx124','dx125','dx126','dx127','dx128','dx129','dx130','dx131','dx132','dx133','dx134','dx135','dx136','dx137','dx138','dx139','dx140','dx141','dx142','dx143','dx144','dx145','dx146','dx147','dx148','dx149','dx151','dx152','dx153','dx154','dx155','dx156','dx157','dx158','dx159','dx160','dx161','dx162','dx163','dx164','dx165','dx166','dx168','dx170','dx171','dx173','dx175','dx178','dx181','dx195','dx197','dx198','dx199','dx200','dx201','dx202','dx203','dx204','dx205','dx206','dx207','dx208','dx209','dx210','dx211','dx212','dx213','dx215','dx217','dx224','dx225','dx226','dx228','dx229','dx230','dx231','dx232','dx233','dx234','dx235','dx236','dx237','dx238','dx239','dx240','dx241','dx242','dx243','dx244','dx245','dx246','dx247','dx248','dx249','dx250','dx251','dx252','dx253','dx255','dx256','dx257','dx259','dx650','dx651','dx652','dx653','dx654','dx655','dx657','dx658','dx659','dx660','dx661','dx662','dx663','dx670','dx2603','dx2607','dx2608','dx2613','dx2615','dx2616','dx2617','dx2618','dx2619','dx2620','dx2621']\n",
    "# lab_dx_to_include = lab_tests_to_include + ['dx10','dx100','dx101','dx102','dx103','dx104','dx105','dx106','dx107','dx108','dx109','dx11','dx110','dx111','dx112','dx113','dx114','dx115','dx116','dx117','dx118','dx119','dx12','dx120','dx121','dx122','dx123','dx124','dx125','dx126','dx127','dx128','dx129','dx13','dx130','dx131','dx132','dx133','dx134','dx135','dx136','dx137','dx138','dx139','dx14','dx140','dx141','dx142','dx143','dx144','dx145','dx146','dx147','dx148','dx149','dx15','dx151','dx152','dx153','dx154','dx155','dx156','dx157','dx158','dx159','dx16','dx160','dx161','dx162','dx163','dx164','dx165','dx166','dx168','dx17','dx170','dx171','dx173','dx175','dx178','dx18','dx181','dx19','dx195','dx197','dx198','dx199','dx2','dx200','dx201','dx202','dx203','dx204','dx205','dx206','dx207','dx208','dx209','dx210','dx211','dx212','dx213','dx215','dx217','dx22','dx224','dx225','dx226','dx228','dx229','dx23','dx230','dx231','dx232','dx233','dx234','dx235','dx236','dx237','dx238','dx239','dx24','dx240','dx241','dx242','dx243','dx244','dx245','dx246','dx247','dx248','dx249','dx25','dx250','dx251','dx252','dx253','dx255','dx256','dx257','dx259','dx26','dx2603','dx2607','dx2608','dx2613','dx2615','dx2616','dx2617','dx2618','dx2619','dx2620','dx2621','dx27','dx29','dx3','dx32','dx33','dx35','dx36','dx37','dx38','dx39','dx4','dx40','dx42','dx43','dx44','dx45','dx46','dx47','dx48','dx49','dx5','dx50','dx51','dx52','dx53','dx54','dx55','dx57','dx58','dx59','dx6','dx60','dx62','dx63','dx64','dx650','dx651','dx652','dx653','dx654','dx655','dx657','dx658','dx659','dx660','dx661','dx662','dx663','dx670','dx7','dx76','dx77','dx78','dx79','dx8','dx80','dx81','dx82','dx83','dx84','dx85','dx86','dx87','dx88','dx89','dx90','dx91','dx92','dx93','dx94','dx95','dx96','dx97','dx98','dx99']\n",
    "lab_dx_to_include = lab_tests_to_include + ['dx10','dx100','dx101','dx102','dx103','dx104','dx105','dx106','dx107','dx108','dx109','dx11','dx110','dx111','dx112','dx113','dx114','dx115','dx116','dx117','dx118','dx119','dx12','dx120','dx121','dx122','dx123','dx124','dx125','dx126','dx127','dx128','dx129','dx13','dx130','dx131','dx132','dx133','dx134','dx135','dx136','dx137','dx138','dx139','dx14','dx140','dx141','dx142','dx143','dx144','dx145','dx146','dx147','dx148','dx149','dx15','dx151','dx152','dx153','dx154','dx155','dx156','dx157','dx158','dx159','dx16','dx160','dx161','dx162','dx163','dx164','dx165','dx166','dx168','dx17','dx170','dx171','dx173','dx175','dx178','dx18','dx181','dx19','dx197','dx198','dx199','dx2','dx200','dx201','dx202','dx203','dx204','dx205','dx206','dx207','dx208','dx209','dx210','dx211','dx212','dx213','dx215','dx217','dx22','dx225','dx226','dx228','dx229','dx23','dx230','dx231','dx232','dx233','dx234','dx235','dx236','dx237','dx238','dx239','dx24','dx241','dx242','dx243','dx244','dx245','dx246','dx247','dx248','dx249','dx25','dx250','dx251','dx252','dx253','dx255','dx256','dx257','dx259','dx26','dx2603','dx2607','dx2608','dx2613','dx2615','dx2616','dx2617','dx2618','dx2619','dx2620','dx2621','dx27','dx29','dx3','dx32','dx33','dx35','dx36','dx37','dx38','dx39','dx4','dx40','dx42','dx43','dx44','dx45','dx46','dx47','dx48','dx49','dx5','dx50','dx51','dx52','dx53','dx54','dx55','dx57','dx58','dx59','dx6','dx60','dx62','dx63','dx64','dx650','dx651','dx652','dx653','dx654','dx655','dx657','dx658','dx659','dx660','dx661','dx662','dx663','dx670','dx7','dx76','dx77','dx78','dx79','dx8','dx80','dx81','dx82','dx83','dx84','dx85','dx86','dx87','dx88','dx89','dx90','dx91','dx92','dx93','dx94','dx95','dx96','dx97','dx98','dx99']\n",
    "\n",
    "\n",
    "for lab_dx in lab_dx_to_include:\n",
    "    agg_dict[lab_dx] = [np.mean]\n",
    "\n",
    "# aggregate so we have 1 row per patient\n",
    "model_data_lab_dx = model_data.groupby(['SUBJECT_ID']).agg(agg_dict).reset_index()\n",
    "\n",
    "# rename columns\n",
    "model_data_lab_dx.columns = ['_'.join(col).strip() for col in model_data_lab_dx.columns.values]\n",
    "model_data_lab_dx = model_data_lab_dx.rename(columns = {'SUBJECT_ID_': 'SUBJECT_ID',\n",
    "                                                    'LOS_amax': 'LOS',\n",
    "                                                    'GENDER_amax': 'GENDER',\n",
    "                                                    'age_amax': 'age',\n",
    "                                                    'is_urgent_amax': 'is_urgent',\n",
    "                                                    'is_emergency_amax': 'is_emergency'})\n",
    "\n",
    "model_data_lab_dx['readmission'] = model_data_lab_dx['readmission_amax']\n",
    "del model_data_lab_dx['readmission_amax']\n",
    "\n",
    "model_data_lab_dx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24089, 254)\n",
      "(6037, 254)\n",
      "(6120, 254)\n"
     ]
    }
   ],
   "source": [
    "# create the lab datasets\n",
    "train_with_lab_dx = model_data_lab_dx.merge(train_patients, on='SUBJECT_ID', how='inner')\n",
    "validation_with_lab_dx = model_data_lab_dx.merge(validation_patients, on='SUBJECT_ID', how='inner')\n",
    "test_with_lab_dx = model_data_lab_dx.merge(test_patients, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(train_with_lab_dx.shape)\n",
    "print(validation_with_lab_dx.shape)\n",
    "print(test_with_lab_dx.shape)\n",
    "train_with_lab_dx.to_csv(data_processed_path + 'train_final.csv', index=False)\n",
    "validation_with_lab_dx.to_csv(data_processed_path + 'validation_final.csv', index=False)\n",
    "test_with_lab_dx.to_csv(data_processed_path + 'test_final.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
